{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8255eb-b647-4621-a848-9e3c65d669f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from collections import deque\n",
    "import argparse\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1158477c-9ef1-49b4-9ab3-74cc9c19c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to enable TensorBoard, or False to disable it\n",
    "use_tensorboard = False\n",
    "\n",
    "if use_tensorboard:\n",
    "    tensorboard = TensorBoard(log_dir=f\"logs/{time.time()}\")\n",
    "else:\n",
    "    tensorboard = None\n",
    "\n",
    "# ... rest of your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b84483a-228d-4d9a-988d-33a1c054ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "\n",
    "    def replay(self, batch_size, tensorboard_callback=None):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0, callbacks=[tensorboard_callback] if tensorboard_callback else [])\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186d3eb-552b-405d-8133-a667719ae19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 00:43:39.892175: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-12-18 00:43:39.892300: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 00:43:39.894057: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-12-18 00:43:39.973101: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-12-18 00:43:39.973300: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2995200000 Hz\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    episodes = 1000\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state_info = env.reset()\n",
    "        state = state_info[0] if isinstance(state_info, tuple) else state_info\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "\n",
    "        # Render the environment every 50 episodes\n",
    "        render = e % 50 == 0\n",
    "\n",
    "        for time_step in range(500):\n",
    "            if render:\n",
    "                %env.render()\n",
    "\n",
    "            action = agent.act(state)\n",
    "            step_info = env.step(action)\n",
    "\n",
    "            if isinstance(step_info, tuple) and len(step_info) >= 4:\n",
    "                next_state, reward, done, _ = step_info[:4]\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected return format from env.step: {step_info}\")\n",
    "\n",
    "            next_state = next_state[0] if isinstance(next_state, tuple) else next_state\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(agent.memory) > 32:\n",
    "                agent.replay(32, tensorboard_callback=tensorboard)\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode: {e}/{episodes}, Reward: {total_reward}, Epsilon: {agent.epsilon}\", flush=True)\n",
    "                break\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            agent.save(f\"./save/lunarlander-dqn-{e}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbf716-5c12-41da-a418-01341dbc4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41732c63-192e-419b-8eb1-c3974122b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent = DQNAgent(state_size, action_size)\n",
    "trained_agent.load(\"./save/lunarlander-dqn.h5\")\n",
    "\n",
    "for e in range(100):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action = trained_agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = np.reshape(next_state, [1, state_size])\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497d34f-23ed-402b-9aaf-cafd0d0ccc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3121459-2036-4400-a4de-6b223a5ba163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
